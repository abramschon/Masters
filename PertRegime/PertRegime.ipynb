{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we just in the perturbative regime?\n",
    "\n",
    "In the 2009 paper [Pairwise Maximum Entropy Models for Studying Large Biological Systems: When They Can Work and When They Can't](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000380), Roudi et al. suggest that when we are in the perturbative regime, characterised by a small mean probability of observing neurons spike and small number of neurons $N$, the pairwise maxent model can appear to be a good model for a distribution. However, we cannot extrapolate the behaviour of the pairwise model to larger $N$, and predict that it will remain a good fit outside of the perturbative regime. In short:\n",
    "\n",
    "> The distance between the pairwise model and **any** probability distribution appears linear in $N\\bar{v}\\delta t$ in the perturbative regime \n",
    "\n",
    "We try and investigate these claims computationally.\n",
    "\n",
    "The perturbative regime is defined as $N\\bar{v}\\delta t \\ll 1$, where $\\bar{v}$ is the mean firing rate and $\\delta t$ is the size of the time bin. As shorthand, we define $\\delta \\doteq \\bar{v}\\delta t$.  For sufficiently small time bins where we observe at most one spike within each bin, and we can identify $\\delta$ with the mean probability of observing a neuron fire. \n",
    "\n",
    "As an example, for $N=5$ neurons, we should be in the perturbative regime with $\\bar{p}(r_i=1)=\\delta \\ll 0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from NumericIsing import Ising\n",
    "from ThreeWise import ThreeWise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try and reproduce the main computational results in the paper. These results show close agreement between the true distance measure between the 'pairwise' and a '3-wise distribution', and an estimate of the distance measure between these distributions, whilst in the perturbative regime. \n",
    "\n",
    "First, we create a '3-wise distribution'. More accurately, we define the interactions in a model that includes up to 3-wise interactions, which creates a distribution that cannot be explained by a model that includes only up to pairwise interactions. \n",
    "\n",
    "This distribution takes the form:\n",
    "$$\n",
    "    P^{(3)}(\\{r_i\\}) = \\frac{1}{Z} \\exp \\left[ \\sum_i h_i r + \\sum_{i< j} J_{ij}r_i r_j  + \\sum_{i<j<k} K_{ijk}r_i r_j r_k  \\right]\n",
    "$$\n",
    "From the paper, the interaction terms were determined as:\n",
    "- $h_i = - \\ln (1/r_i^* - 1)$, where each $r_i^*$ is randomly drawn from an exponential distribution with mean 0.02\n",
    "- $J_{ij}$ is drawn from a Gaussian distribution with mean 0.05 and s.d. 0.8\n",
    "- $K_{ijk}$ is drawn from a Gaussian distribution with mean 0.02 and s.d. 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_p_true(N):\n",
    "    \"\"\"\n",
    "    Function that creates a distribution that includes up to 3-wise interactions,\n",
    "    by randomly sampling the interaction weights from the distributions specified in the paper\n",
    "    \"\"\"\n",
    "    h = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        h[i] = -np.log(1/np.random.exponential(0.02) - 1)\n",
    "        \n",
    "    J = np.zeros((N,N))\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            J[i,j] = np.random.normal(0.05, 0.8)\n",
    "            \n",
    "    K = np.zeros((N,N,N))\n",
    "    for i in range(N-2):\n",
    "        for j in range(i+1,N-1):\n",
    "            for k in range(j+1,N):\n",
    "                K[i,j,k] = np.random.normal(0.02, 0.5)\n",
    "    \n",
    "    p_true = ThreeWise(N, h, J, K)\n",
    "    return p_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 0,1,2\n",
      " 9.004156606172887e-06\n",
      "Correlation between 0,1,3\n",
      " 5.097922331429626e-08\n",
      "Correlation between 0,2,3\n",
      " 1.7118079779222967e-08\n",
      "Correlation between 1,2,3\n",
      " 1.4737371445570076e-06\n"
     ]
    }
   ],
   "source": [
    "p_true = sim_p_true(4)\n",
    "#calc 3 wise correlations\n",
    "for i in range(p_true.N-2):\n",
    "    for j in range(i+1,p_true.N-1):\n",
    "        for k in range(j+1, p_true.N):\n",
    "            print(f\"Correlation between {i},{j},{k}\\n\",p_true.expectation(lambda s: s[0]*s[1]*s[2], [i,j,k], p_true.p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main results in the paper are, in the perturbative regime:\n",
    "- $\\Delta_N = D_{K L}\\left(p_{\\text {true }} \\| p_{\\text {pair }}\\right) / D_{K L}\\left(p_{\\text {true }} \\| p_{\\text {ind }}\\right)$ scales as $(N-2)\\delta + \\mathcal{O}\\left((N \\delta)^{2}\\right)$ (I assume the power of 2 is just a safe upper bound). \n",
    "- $$D_{K L}\\left(p_{\\text {true }} \\| p_{\\text {pair }}\\right)=\\frac{1}{\\ln 2} \\sum_{i<j<k} \\bar{r}_{i} \\bar{r}_{j} \\bar{r}_{k} f\\left(\\bar{\\boldsymbol{\\rho}}_{ijk}^{\\text{true}}, \\bar{\\boldsymbol{\\rho}}_{i j k}^{\\text{pair}}\\right)+\\mathcal{O}\\left((N \\delta)^{4}\\right)$$\n",
    "- $$D_{K L}\\left(p_{\\text {true }} \\| p_{\\text {ind }}\\right)=\\frac{1}{\\ln 2} \\sum_{i<j} \\bar{r}_{i} \\bar{r}_{j} f\\left(\\rho_{i j}^{\\text{true}}, 0\\right)+\\mathcal{O}\\left((N \\delta)^{3}\\right)$$\n",
    "\n",
    "where $f(x,y) \\doteq (1+x)\\big[\\ln(1+x)-\\ln(1+y)\\big] - (x-y)$, $\\overline{r}_i = \\langle r \\rangle $ (they use $r$ instead of $\\sigma$ in the paper), $\\rho_{ij}$ is the normalised pairwise correlation defined as:\n",
    "$$\n",
    "    \\rho_{ij} \\doteq \\frac{\\langle r_i r_j \\rangle - \\bar{r_i} \\bar{r_j}}{\\bar{r_i} \\bar{r_j}}\n",
    "$$\n",
    "and $\\bar{\\boldsymbol{\\rho}}_{i j k}^{p}$ is defined as:\n",
    "$$\n",
    "    \\bar{\\boldsymbol{\\rho}}_{i j k}^{p} \\doteq \\frac{\\langle r_i r_j r_k \\rangle - \\bar{r_i} \\bar{r_j} \\bar{r_k}}{\\bar{r_i} \\bar{r_j} \\bar{r_k}}\n",
    "$$\n",
    "Note, $\\bar{\\boldsymbol{\\rho}}_{i j k}^{p}$ depends on the distribution $p$ that is used to calculate the expectations.\n",
    "\n",
    "The KL divergence is defined as $D_{KL}(p\\| q) \\doteq \\sum_s p_s \\ln (p_s/q_s) $, where we sum over all states $s$, which in this case is defined as all configurations of spins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(p):\n",
    "    avgs = p.averages() #get the averages \\bar{r}\n",
    "    corrs_2 = p.correlations() #get the pairwise correlations <r_i r_j>\n",
    "    \n",
    "    N = p.N #how many neurons\n",
    "    \n",
    "    #calc all the \\rho_{ij}\n",
    "    rho = np.zeros((N,N))\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            rho[i,j]= corrs_2[i,j]/(avgs[i]*avgs[j]) - 1\n",
    "            \n",
    "    #calc all the \\bar{\\rho}_{ijk}\n",
    "    rho_b = np.zeros((N,N,N))\n",
    "    corrs_3 = np.zeros((N,N,N))\n",
    "    for i in range(N-2):\n",
    "        for j in range(i+1,N-1):\n",
    "            for k in range(j+1,N):\n",
    "                corrs_3[i,j,k]= p.expectation(lambda s: s[0]*s[1]*s[2], [i,j,k], p.p) \n",
    "                rho_b[i,j,k] = corrs_3[i,j,k] / (avgs[i]*avgs[j]*avgs[k]) - 1\n",
    "                \n",
    "    return avgs, corrs_2, corrs_3, rho, rho_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test all of this out, let's define a model that models the probability of each neuron firing as being independent. This corresponds to setting the 'local fields' $h_i$ to $\\ln(\\bar{r}_i/(1-\\bar{r}_i))$, and all other interaction parameters to 0. The expectations $\\langle r_i r_j ... r_p \\rangle$ should purely be products of the expectations of seeing indivdual neurons fire $\\bar{r}_i \\bar{r}_j ... \\bar{r}_p$. Hence, the normalised correlations should all be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "r_bar = np.ones(N)*0.5\n",
    "h = -np.log(1/r_bar - 1)\n",
    "J = np.zeros((N,N))\n",
    "K = np.zeros((N,N,N))\n",
    "\n",
    "p_ind = ThreeWise(N,h,J,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 0.5, 0.5, 0.5]),\n",
       " array([[0.5 , 0.25, 0.25, 0.25],\n",
       "        [0.  , 0.5 , 0.25, 0.25],\n",
       "        [0.  , 0.  , 0.5 , 0.25],\n",
       "        [0.  , 0.  , 0.  , 0.5 ]]),\n",
       " array([[[0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.125, 0.125],\n",
       "         [0.   , 0.   , 0.   , 0.125],\n",
       "         [0.   , 0.   , 0.   , 0.   ]],\n",
       " \n",
       "        [[0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.125],\n",
       "         [0.   , 0.   , 0.   , 0.   ]],\n",
       " \n",
       "        [[0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ]],\n",
       " \n",
       "        [[0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , 0.   ]]]),\n",
       " array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]),\n",
       " array([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statistics(p_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling toy distributions with the pairwise model\n",
    "\n",
    "We consider some toy distributions, one that has all neurons firing or not (\"All or nothing model\"), another which only takes on 2 states (\"A couple of states or nothing model\") and a third where one of the neurons only fires if only one of the other 2 neurons fire (\"XOR\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All or nothing model\n",
    "We start by considering a very simple distribution that has higher order correlations. Let us say we have 5 neurons which always fire in sync. Thus $p(1,1,1,1,1) = c$, $p(0,0,0,0,0)=(1-c)$ and all other events have probability 0. The mean firing rate of individual neurons will be c, as will the correlations. We will vary the probability $c$ of all of them firing, and see whether the pairwise model appears to be a good fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0.5 0.5 0.5]\n",
      "[[0.  0.5 0.5 0.5 0.5]\n",
      " [0.  0.  0.5 0.5 0.5]\n",
      " [0.  0.  0.  0.5 0.5]\n",
      " [0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "c = 0.5\n",
    "avgs = c*np.ones(N) # prob of every neuron firing in a window is 0.5\n",
    "corrs = c*np.triu(np.ones((N,N)),1) # prob of 2 neurons firing in the same window is 0.2 \n",
    "print(avgs,corrs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise = Ising(N, avgs, corrs, lr=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise.gradient_ascent() # 100 steps of gradient ascent. Repeat until accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted averages:\n",
      "[0.53619501 0.53738947 0.54056568 0.54641463 0.55341676]\n",
      "Predicted correlations:\n",
      "[[0.53619501 0.51575932 0.51198993 0.50910874 0.50459316]\n",
      " [0.         0.53738947 0.505844   0.502652   0.49986001]\n",
      " [0.         0.         0.54056568 0.50027544 0.49694551]\n",
      " [0.         0.         0.         0.54641463 0.49396977]\n",
      " [0.         0.         0.         0.         0.55341676]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted averages:\", p_wise.averages(), \"Predicted correlations:\", p_wise.correlations(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a maximum entropy model, let us see what it thinks the true probability distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.] 0.37\n",
      "[1. 1. 1. 1. 1.] 0.46\n"
     ]
    }
   ],
   "source": [
    "for state in [p_wise.states[0],p_wise.states[-1]]:\n",
    "    print(state,np.round(p_wise.p(state),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the pairwise model is able to accurately predict the full probability distribution of the 'all or nothing model' for different values of $c$. I honestly wasn't sure what to expect here, and would be interested in relating this to the results from the Roudi et al. paper. We will have to consider slightly more complex distributions to 'break' the pairwise model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple of states or nothing\n",
    "The next model that came to mind that takes on two states:\n",
    "\n",
    "- $p(0,1,1,1,1)=a$\n",
    "- $p(1,1,1,1,0)=b$\n",
    "- $p(0,0,0,0,0)=1-(a+b)$\n",
    "\n",
    "We define $a+b \\doteq c$\n",
    "\n",
    "The expectation of the neurons will be: \n",
    "\n",
    "    (b, c, c, c, c, a)\n",
    "\n",
    "The pairwise correlations will be:\n",
    "\n",
    "        1 2 3 4 5\n",
    "      1   b b b 0\n",
    "      2     c c a    \n",
    "      3       c a\n",
    "      4         a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.6 0.6 0.6 0.2]\n",
      "[[0.  0.4 0.4 0.4 0. ]\n",
      " [0.  0.  0.6 0.6 0.2]\n",
      " [0.  0.  0.  0.6 0.2]\n",
      " [0.  0.  0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "a = 0.2\n",
    "b = 0.4\n",
    "c = a + b\n",
    "avgs = np.array([b, c, c, c, a])\n",
    "corrs = np.array([[0,b,b,b,0],\n",
    "                  [0,0,c,c,a],\n",
    "                  [0,0,0,c,a],\n",
    "                  [0,0,0,0,a]])\n",
    "print(avgs,corrs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise = Ising(N, avgs, corrs, lr=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise.gradient_ascent() # 100 steps of gradient ascent. Repeat until accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted averages:\n",
      "[0.40120964 0.60166855 0.60201396 0.60243361 0.20180175]\n",
      "Predicted correlations:\n",
      "[[0.40120964 0.40017571 0.40004125 0.39989105 0.00277631]\n",
      " [0.         0.60166855 0.60019171 0.59988656 0.19942072]\n",
      " [0.         0.         0.60201396 0.59956424 0.19928799]\n",
      " [0.         0.         0.         0.60243361 0.19916612]\n",
      " [0.         0.         0.         0.         0.20180175]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted averages:\", p_wise.averages(), \"Predicted correlations:\", p_wise.correlations(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.] 0.392\n",
      "[0. 0. 0. 0. 1.] 0.002\n",
      "[0. 0. 0. 1. 0.] 0.002\n",
      "[0. 0. 0. 1. 1.] 0.0\n",
      "[0. 0. 1. 0. 0.] 0.001\n",
      "[0. 0. 1. 0. 1.] 0.0\n",
      "[0. 0. 1. 1. 0.] 0.0\n",
      "[0. 0. 1. 1. 1.] 0.0\n",
      "[0. 1. 0. 0. 0.] 0.0\n",
      "[0. 1. 0. 0. 1.] 0.0\n",
      "[0. 1. 0. 1. 0.] 0.0\n",
      "[0. 1. 0. 1. 1.] 0.0\n",
      "[0. 1. 1. 0. 0.] 0.0\n",
      "[0. 1. 1. 0. 1.] 0.001\n",
      "[0. 1. 1. 1. 0.] 0.004\n",
      "[0. 1. 1. 1. 1.] 0.196\n",
      "[1. 0. 0. 0. 0.] 0.001\n",
      "[1. 0. 0. 0. 1.] 0.0\n",
      "[1. 0. 0. 1. 0.] 0.0\n",
      "[1. 0. 0. 1. 1.] 0.0\n",
      "[1. 0. 1. 0. 0.] 0.0\n",
      "[1. 0. 1. 0. 1.] 0.0\n",
      "[1. 0. 1. 1. 0.] 0.0\n",
      "[1. 0. 1. 1. 1.] 0.0\n",
      "[1. 1. 0. 0. 0.] 0.0\n",
      "[1. 1. 0. 0. 1.] 0.0\n",
      "[1. 1. 0. 1. 0.] 0.0\n",
      "[1. 1. 0. 1. 1.] 0.0\n",
      "[1. 1. 1. 0. 0.] 0.001\n",
      "[1. 1. 1. 0. 1.] 0.0\n",
      "[1. 1. 1. 1. 0.] 0.396\n",
      "[1. 1. 1. 1. 1.] 0.003\n"
     ]
    }
   ],
   "source": [
    "for state in p_wise.states:\n",
    "    print(state,np.round(p_wise.p(state),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the pairwise model is able to capture the probability distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR \n",
    "\n",
    "In the 2003 Schneidman paper, *Network Information and Connected Correlations*, they say,\n",
    "\n",
    "> If $\\sigma_3$ is formed as the exclusive OR (XOR) of the variables $\\sigma_1$ and $\\sigma_2$, then the essential structure of $p(\\sigma_1,\\sigma_2,\\sigma_3)$ is contained in a three–spin interaction. \n",
    "\n",
    "This might give us a simple example of something the ising model can't model.\n",
    "\n",
    "Let us say that $\\sigma_1$ and $\\sigma_2$ firing independently with probabilities $p(\\sigma_1{=}1)=a$ and $p(\\sigma_2{=}1)=b$. \n",
    "\n",
    "        s_1 s_2 s_3  p(s_1, s_2, s_3)\n",
    "        0   0   0    (1-a)(1-b)\n",
    "        0   1   1    (1-a)b\n",
    "        1   0   1    a(1-b)\n",
    "        1   1   0    ab\n",
    "Thus, the averages are:\n",
    "\n",
    "        (a, b, b+a-2ab)\n",
    "\n",
    "And the correlations are:\n",
    "\n",
    "        s_1 s_2, s_1 s_3, s_2,s_3\n",
    "        ab       a(1-b)   (1-a)b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2  0.4  0.44]\n",
      "[[0.   0.08 0.12]\n",
      " [0.   0.   0.32]]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "a = 0.2\n",
    "b = 0.4\n",
    "avgs = np.array([a,b,b+a-2*a*b])\n",
    "corrs = np.array([[0,a*b,a*(1-b)],\n",
    "                  [0,0,(1-a)*b]])\n",
    "print(avgs,corrs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise = Ising(3, avgs, corrs, lr=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wise.gradient_ascent() # 100 steps of gradient ascent. Repeat until accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted averages:\n",
      "[0.20000128 0.40000042 0.44000421]\n",
      "Predicted correlations:\n",
      "[[0.20000128 0.08000851 0.11999224]\n",
      " [0.         0.40000042 0.31999632]\n",
      " [0.         0.         0.44000421]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted averages:\", p_wise.averages(), \"Predicted correlations:\", p_wise.correlations(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.] 0.406\n",
      "[0. 0. 1.] 0.074\n",
      "[0. 1. 0.] 0.074\n",
      "[0. 1. 1.] 0.246\n",
      "[1. 0. 0.] 0.074\n",
      "[1. 0. 1.] 0.046\n",
      "[1. 1. 0.] 0.006\n",
      "[1. 1. 1.] 0.074\n"
     ]
    }
   ],
   "source": [
    "for state in p_wise.states:\n",
    "    print(state,np.round(p_wise.p(state),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0] 0.48\n",
      "[0, 1, 1] 0.32000000000000006\n",
      "[1, 0, 1] 0.12\n",
      "[1, 1, 0] 0.08000000000000002\n"
     ]
    }
   ],
   "source": [
    "print([0,0,0], (1-a)*(1-b))\n",
    "print([0,1,1], (1-a)*b)\n",
    "print([1,0,1], a*(1-b))\n",
    "print([1,1,0], a*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the events `[0,0,1]` and `[1,1,1]` are assigned non-zero probabilities, when they should in fact be zero. In general, we can see the predictions are far off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python39264bit3926084c7cc7f3d4f4391856d584bc48b87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
