{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39264bitvenvvenv098b745301f24a4a9273629fd09e163f",
   "display_name": "Python 3.9.2 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Determining the Ising model numerically\n",
    "For a good review of what this is all about, see [Inverse statistical problems: from the inverse Ising problem to data science](https://arxiv.org/abs/1702.01522). We want to solve for the variables $\\boldsymbol{h}\\in\\mathbb{R}^{N}$ and $\\boldsymbol{J}\\in\\mathbb{R}^{N\\times(N-1)/2}$ such that our Ising model reproduces observed averages $\\langle \\sigma_i \\rangle^D$ and correlations $\\langle \\sigma_i \\sigma_j \\rangle ^D$. We start by taking a na√Øve approach, using gradient ascent on the log-likelihood function:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_{D}(\\boldsymbol{h}, \\boldsymbol{J}) &=\\frac{1}{M} \\ln p(\\mathrm{D} \\mid \\boldsymbol{J}, \\boldsymbol{h}) \\\\\n",
    "&=\\sum_{i<j} J_{i j}\\left\\langle\\sigma_{i} \\sigma_{j}\\right\\rangle^{D}+\\sum_{i} h_{i}\\left\\langle\\sigma_{i}\\right\\rangle^{D}-\\ln Z(\\boldsymbol{h}, \\boldsymbol{J})\n",
    "\\end{aligned}\n",
    "$$\n",
    "The partial derivatives of the log-likelihood function are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_{D}}{\\partial h_i} =& \\langle \\sigma_i \\rangle^D - \\langle \\sigma_i \\rangle \\\\\n",
    "\\frac{\\partial L_{D}}{\\partial J_{ij}} =& \\langle \\sigma_i \\sigma_j  \\rangle^D - \\langle \\sigma_i \\sigma_j \\rangle\n",
    "\\end{aligned}\n",
    "$$\n",
    "and the gradient ascent update rule is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_i^{(t+1)} =& h_i^{(t)} + \\lambda \\frac{\\partial L_{D}}{\\partial h_i} \\\\\n",
    "J_{ij}^{(t+1)} =& J_{ij}^{(t)} + \\lambda \\frac{\\partial L_{D}}{\\partial J_{ij}} \n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\lambda$ is the learning rate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-30-be85bb733a04>, line 98)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-be85bb733a04>\"\u001b[0;36m, line \u001b[0;32m98\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Ising:\n",
    "    \"\"\"\n",
    "    Represents an Ising model.\n",
    "    Variables:\n",
    "        N - no. spins\n",
    "        av_s - vector of expectations for each spin\n",
    "        av_ss - matrix of pairwise correlations\n",
    "        lr - learning rate\n",
    "        spin_vals - the values each spin takes on, typically [0,1] or [-1,1]\n",
    "        states - matrix of all possible states\n",
    "        h - vector of the local magnetic fields \n",
    "        J - matrix of the pairwise couplings \n",
    "        Z - the current value of the partition function\n",
    "    \"\"\"\n",
    "    def __init__(self, N, avgs, corrs, lr=0.1, spin_vals=[0,1]):\n",
    "        # set user input\n",
    "        self.N = N\n",
    "        self.avgs = avgs\n",
    "        self.corrs = corrs\n",
    "        self.lr = lr\n",
    "        self.spin_vals = spin_vals\n",
    "        # determine all states\n",
    "        self.states = np.array([self.to_binary(n) for n in range(2**N)]) \n",
    "        # randomly initialise h and J\n",
    "        self.h = np.random.random_sample((N))\n",
    "        self.J = np.triu( np.random.random_sample((N,N)), 1)\n",
    "        # work out the partition function Z\n",
    "        self.Z = self.calc_Z()\n",
    "    \n",
    "    # Methods for calculating probabilities and expectations\n",
    "    def expectation(self, f, ind, p):\n",
    "        \"\"\"\n",
    "        Returns the sum over all states of the function f, weighted by p. \n",
    "        Args:\n",
    "            f - a function of a subset of the spins\n",
    "            ind - indices of the spins which are involved in f\n",
    "            p - a function of the state, for instance the probability p of observing the state\n",
    "        \"\"\"\n",
    "        exp = 0\n",
    "        for s in self.states:\n",
    "            exp += f( [s[i] for i in ind] ) * p(s)\n",
    "        \n",
    "        return exp\n",
    "\n",
    "    def p(self, s):\n",
    "        \"\"\"\n",
    "        Returns the normalized probability of the state s given the model parameters \n",
    "        Args:\n",
    "            s - np.array of the state, e.g. np.array([0,0,1]), here the third neuron fires\n",
    "        \"\"\"\n",
    "        return np.exp(-self.H(s)) / self.Z\n",
    "\n",
    "    def p_unnormalized(self, s):\n",
    "        \"\"\"\n",
    "        Returns the unnormalized probability (not divided Z) of the state s given the model parameters \n",
    "        Args:\n",
    "            s - np.array of the state\n",
    "        \"\"\"\n",
    "        return np.exp(-self.H(s))\n",
    "\n",
    "    def H(self, s):\n",
    "        \"\"\"\n",
    "        Return the hamiltonian H(s) of the state s\n",
    "        Args:\n",
    "            s - np.array of the state\n",
    "        \"\"\"\n",
    "        return -self.h.dot(s) - s@self.J@s \n",
    "            \n",
    "    def calc_Z(self):\n",
    "        \"\"\" \n",
    "        Calculates the partition function Z based on the current h and J.\n",
    "        \"\"\"\n",
    "        # (the lambda function just returns 1 since this is just a sum of p over all states) \n",
    "        Z = self.expectation(lambda args: 1, [], self.p_unnormalized) \n",
    "        return Z \n",
    "    \n",
    "    def to_binary(self, n):\n",
    "        \"\"\"\n",
    "        Returns a binary rep of the int n as an array of size N, e.g. Assuming N = 5, 3 -> np.array([0,0,0,1,1]) \n",
    "        \"\"\"\n",
    "        b = np.zeros(self.N)\n",
    "        for i in range(N):\n",
    "            if n % 2 == 1: b[N-1-i]=1 # index N-1-i otherwise numbers are reversed\n",
    "            n//=2\n",
    "            if n==0: break\n",
    "        return b\n",
    "\n",
    "    # Methods for gradient ascent\n",
    "    def gradient_ascent(self):\n",
    "        \"\"\"\n",
    "        Performs gradient ascent on the log-likelihood and updates h and J\n",
    "        \"\"\"\n",
    "        steps = 100\n",
    "        for n in range(steps): #update this condition to check accuracy\n",
    "            # work out corrections to h\n",
    "            h_new = self.h\n",
    "            for i in range(self.N):\n",
    "                pass\n",
    "\n",
    "            # work out corrections to J\n",
    "            J_new = self.J\n",
    "            for i in range(self.N-1):\n",
    "                for j in range(i+1,N):\n",
    "                    pass\n",
    "\n",
    "            # perform the update\n",
    "            self.h = h_new\n",
    "            self.J = J_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 1, 0 2, 0 3, \n1 2, 1 3, \n2 3, \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        print(i,j,end=\", \")\n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Create and train an Ising model on some fictional data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.5 0.5 0.5 0.5]\n[[0.  0.2 0.2 0.2]\n [0.  0.  0.2 0.2]\n [0.  0.  0.  0.2]\n [0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "avgs = 0.5*np.ones(N) # prob of every neuron firing in a window is 0.5\n",
    "corrs = 0.2*np.triu(np.ones((N,N)),1) # prob of 2 neurons firing in the same window is 0.2 \n",
    "print(avgs,corrs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'N': 4,\n",
       " 'avgs': array([0.5, 0.5, 0.5, 0.5]),\n",
       " 'corrs': array([[0. , 0.2, 0.2, 0.2],\n",
       "        [0. , 0. , 0.2, 0.2],\n",
       "        [0. , 0. , 0. , 0.2],\n",
       "        [0. , 0. , 0. , 0. ]]),\n",
       " 'lr': 0.1,\n",
       " 'spin_vals': [0, 1],\n",
       " 'states': array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 1., 1., 0.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 1.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'h': array([0.14756489, 0.92124773, 0.04867979, 0.71479699]),\n",
       " 'J': array([[0.        , 0.23619783, 0.61125111, 0.92269517],\n",
       "        [0.        , 0.        , 0.44777157, 0.27042526],\n",
       "        [0.        , 0.        , 0.        , 0.04394949],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]),\n",
       " 'Z': 170.98697504882213}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "ising = Ising(N, avgs, corrs)\n",
    "ising.__dict__ # all the class variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4597798200658618"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "ising.p(np.ones(N)) #probability of all neurons firing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}